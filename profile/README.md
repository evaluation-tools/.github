## Evaluation at the Forefront

The mission is to bring sustainable, well-maintained safety and evaluation practices to AI practitioners to ensure that we know what our powerful AI systems are capable of.

## Research Artifacts
### Tools
- [Nutcracker](https://github.com/evaluation-tools/nutcracker)
  - ![GitHub Repo stars](https://img.shields.io/github/stars/evaluation-tools/nutcracker?style=flat&logo=github&labelColor=%23696969&color=%23708090) ![PyPI - Downloads](https://img.shields.io/pypi/dm/nutcracker?logo=pypi&labelColor=%23696969&color=%23708090) 
  - Evaluate API-served LLMs / LLM applications
- [Nutcracker DB](https://github.com/evaluation-tools/nutcracker-db)
  - ![GitHub Repo stars](https://img.shields.io/github/stars/evaluation-tools/nutcracker-db?style=flat&logo=github&labelColor=%23696969&color=%23708090)
  - A supporting database for Nutcracker (separately maintained for academic reproducibility purposes)
  - Currently supports 90+ MCQ / FRQ benchmarks
